{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM              # new! \n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')                  # chop into 80 character chunks \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))    # HoLY SHIT, that's all that's needed!!! \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM              # new! \n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 2]),\n",
       "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### RNN Text generation with NumPy\n",
    "\n",
    "What else can we do with RNN? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. We'll pull some news stories using [newspaper](https://github.com/codelucas/newspaper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz1m55G5WSrQ"
   },
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "ahlHBeoZCaLX",
    "outputId": "7b1c5f93-3fa5-42db-acb6-3c894b0accef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.22.0)\n",
      "Collecting Pillow>=3.3.0 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/66/6113477dc3206ccb1e192cffd626f2840ead02375a6cebe2436ad4c19f61/Pillow-6.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 56.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting feedparser>=5.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 56.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/90/18ac0e5340b6228c25cc8e79835c3811e7553b2b9ae87296dfeb62b7866d/tldextract-2.2.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 22.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml>=3.6.0 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/be/5ab8abdd8663c0386ec2dd595a5bc0e23330a0549b8a91e32f38c20845b6/lxml-4.4.1-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8MB 57.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4>=4.4.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/c8/a55eb6ea11cd7e5ac4bacdf92bac4693b90d3ba79268be16527555e186f0/beautifulsoup4-4.8.1-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 42.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting nltk>=3.2.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 68.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4MB 48.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (5.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.10.0->newspaper3k) (2.6)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.11.0)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (41.2.0)\n",
      "Collecting soupsieve>=1.2 (from beautifulsoup4>=4.4.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/42/d821581cf568e9b7dfc5b415aa61952b0f5e3dede4f3cbd650e3a1082992/soupsieve-1.9.4-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: feedparser, feedfinder2, nltk, tinysegmenter, jieba3k\n",
      "  Building wheel for feedparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44725 sha256=4103ba5170f517aca841d52478155ff07e152abb075edb5f9144c3aa52e6273f\n",
      "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=4727 sha256=b2ca1e6b2b88f1dda4ad01571b0d056ffdbe750e9eeb97ece7132f5146e0eaba\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1450720 sha256=25a16075199353bfb105b00e7ba06c37b7bab980c36c1247447bcc2a620f11f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=14367 sha256=4bbfd47a9fff3162b83f6ad5b3e9967a40fdd262b185ef0516b843662c5fed95\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7399105 sha256=bb25fcdba6830ae04ed661fd4fd4736bd21a169fbe309a75dfd9904d81a00747\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "Successfully built feedparser feedfinder2 nltk tinysegmenter jieba3k\n",
      "Installing collected packages: Pillow, feedparser, cssselect, soupsieve, beautifulsoup4, feedfinder2, requests-file, tldextract, lxml, nltk, tinysegmenter, jieba3k, newspaper3k\n",
      "Successfully installed Pillow-6.2.0 beautifulsoup4-4.8.1 cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 lxml-4.4.1 newspaper3k-0.2.8 nltk-3.4.5 requests-file-1.4.3 soupsieve-1.9.4 tinysegmenter-0.3 tldextract-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTPlziljCiNJ"
   },
   "outputs": [],
   "source": [
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bk9JF2zaCxoO",
    "outputId": "9e66fc15-a397-4b59-f810-d2182565c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = newspaper.build('https://www.apnews.com')\n",
    "len(ap.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Vc6JgAIJDF4E",
    "outputId": "44a13922-d86a-4668-c4fd-455c0d03b6c1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3e9c7a3044b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ap' is not defined"
     ]
    }
   ],
   "source": [
    "article_text = ''\n",
    "\n",
    "for article in ap.articles[:1]:\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article_text += '\\n\\n' + article.text\n",
    "    except:\n",
    "        print('Failed: ' + article.url)\n",
    "\n",
    "    article_text = article_text.split('\\n\\n')[1]\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = []\n",
    "\n",
    "import os\n",
    "\n",
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_text = []\n",
    "# for filename in data_files:\n",
    "#    if filename[-3:] == 'txt':\n",
    "#        path = f'./articles/{filename}'\n",
    "#        #print(path)\n",
    "#        with open(path, 'r') as data:\n",
    "#            content = data.read()\n",
    "#            #print(content)\n",
    "#            article_text.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = []\n",
    "\n",
    "for filename in data_files:\n",
    "    if filename[-3:] == 'txt':\n",
    "        path = os.path.join('./articles', filename)\n",
    "        with open(path, 'r') as data:\n",
    "            content = data.read()\n",
    "            article_text.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some recent headlines from schools around'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = [a[:50] for a in article_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rsMBBMcv_nRM",
    "outputId": "9f77b07b-4a5a-4ac8-f1b3-79e1a5331fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters :  78\n",
      "txt_data_size :  6935\n"
     ]
    }
   ],
   "source": [
    "# Based on \"The Unreasonable Effectiveness of RNN\" implementation\n",
    "import numpy as np\n",
    "\n",
    "article_text = \" \".join(article_text)\n",
    "chars = list(set(article_text)) # split and remove duplicate characters. convert to list.\n",
    "\n",
    "num_chars = len(chars) # the number of unique characters\n",
    "txt_data_size = len(article_text)\n",
    "\n",
    "print(\"unique characters : \", num_chars)\n",
    "print(\"txt_data_size : \", txt_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "aQygqc_CAWRA",
    "outputId": "30c45e95-057a-4643-9cae-fc518b49c914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, '6': 1, 'C': 2, '’': 3, ':': 4, 'H': 5, '1': 6, 'Y': 7, 'P': 8, \"'\": 9, 'U': 10, 'c': 11, '4': 12, 'a': 13, 'á': 14, '\\n': 15, 'p': 16, '”': 17, 'M': 18, '2': 19, '5': 20, 'q': 21, '.': 22, 'w': 23, ' ': 24, 'O': 25, '“': 26, '/': 27, 'J': 28, 'j': 29, 'W': 30, 'm': 31, 'l': 32, 'I': 33, 'b': 34, 'n': 35, 'g': 36, 'K': 37, ')': 38, 'N': 39, 'G': 40, 'y': 41, 'i': 42, 'Q': 43, 'k': 44, '7': 45, 't': 46, '+': 47, 'V': 48, 'A': 49, '&': 50, '!': 51, 'D': 52, '-': 53, 'h': 54, '—': 55, 'L': 56, ',': 57, 'x': 58, '8': 59, 'f': 60, 'z': 61, 'R': 62, 'Z': 63, 'T': 64, 'S': 65, 'B': 66, 'E': 67, '(': 68, 'F': 69, 'v': 70, 'e': 71, 'r': 72, '9': 73, 'd': 74, 'u': 75, '0': 76, 's': 77}\n",
      "----------------------------------------------------\n",
      "{0: 'o', 1: '6', 2: 'C', 3: '’', 4: ':', 5: 'H', 6: '1', 7: 'Y', 8: 'P', 9: \"'\", 10: 'U', 11: 'c', 12: '4', 13: 'a', 14: 'á', 15: '\\n', 16: 'p', 17: '”', 18: 'M', 19: '2', 20: '5', 21: 'q', 22: '.', 23: 'w', 24: ' ', 25: 'O', 26: '“', 27: '/', 28: 'J', 29: 'j', 30: 'W', 31: 'm', 32: 'l', 33: 'I', 34: 'b', 35: 'n', 36: 'g', 37: 'K', 38: ')', 39: 'N', 40: 'G', 41: 'y', 42: 'i', 43: 'Q', 44: 'k', 45: '7', 46: 't', 47: '+', 48: 'V', 49: 'A', 50: '&', 51: '!', 52: 'D', 53: '-', 54: 'h', 55: '—', 56: 'L', 57: ',', 58: 'x', 59: '8', 60: 'f', 61: 'z', 62: 'R', 63: 'Z', 64: 'T', 65: 'S', 66: 'B', 67: 'E', 68: '(', 69: 'F', 70: 'v', 71: 'e', 72: 'r', 73: '9', 74: 'd', 75: 'u', 76: '0', 77: 's'}\n",
      "----------------------------------------------------\n",
      "[5, 71, 72, 71, 24, 13, 72, 71, 24, 77, 0, 31, 71, 24, 72, 71, 11, 71, 35, 46, 24, 54, 71, 13, 74, 32, 42, 35, 71, 77, 24, 60, 72, 0, 31, 24, 77, 11, 54, 0, 0, 32, 77, 24, 13, 72, 0, 75, 35, 74, 24, 64, 5, 67, 24, 56, 33, 40, 5, 64, 66, 10, 56, 66, 15, 15, 67, 35, 71, 72, 36, 41, 24, 65, 71, 11, 72, 71, 46, 13, 72, 41, 24, 62, 42, 11, 44, 24, 8, 71, 72, 72, 41, 22, 24, 68, 49, 8, 24, 8, 54, 24, 64, 54, 71, 24, 11, 13, 75, 77, 71, 24, 23, 13, 77, 24, 11, 13, 72, 74, 42, 13, 11, 24, 13, 72, 72, 71, 77, 46, 57, 24, 77, 13, 42, 74, 24, 54, 42, 77, 24, 32, 0, 35, 36, 46, 42, 31, 71, 24, 23, 72, 24, 26, 8, 54, 42, 32, 32, 42, 16, 77, 24, 31, 13, 41, 24, 23, 13, 35, 46, 24, 75, 77, 24, 46, 0, 24, 46, 54, 42, 35, 44, 24, 54, 71, 3, 77, 24, 36, 42, 70, 42, 35, 36, 24, 75, 77, 24, 13, 24, 31, 0, 24, 25, 16, 42, 35, 42, 0, 35, 24, 23, 72, 42, 46, 71, 72, 15, 15, 30, 54, 71, 35, 24, 66, 13, 72, 13, 11, 44, 24, 25, 34, 13, 31, 13, 24, 23, 13, 77, 24, 16, 72, 71, 77, 42, 74, 71, 35, 46, 57, 24, 54, 24, 49, 24, 23, 54, 42, 46, 71, 24, 69, 0, 72, 46, 24, 30, 0, 72, 46, 54, 24, 16, 0, 32, 42, 11, 71, 24, 0, 60, 60, 42, 11, 71, 72, 24, 60, 13, 46, 13, 32, 32, 41, 24, 77, 54, 0, 46, 24, 13, 24, 34, 24, 15, 15, 49, 24, 11, 32, 13, 77, 77, 42, 11, 24, 23, 13, 41, 24, 46, 0, 24, 32, 42, 71, 24, 23, 42, 46, 54, 24, 11, 54, 13, 72, 46, 77, 24, 42, 77, 24, 34, 41, 24, 77, 46, 13, 72, 46, 42, 35, 36, 24, 24, 64, 54, 72, 0, 75, 36, 54, 0, 75, 46, 24, 31, 41, 24, 32, 42, 60, 71, 57, 24, 33, 3, 70, 71, 24, 34, 71, 71, 35, 24, 46, 71, 13, 77, 71, 74, 24, 34, 41, 24, 0, 46, 54, 71, 72, 24, 23, 0, 31, 71, 24, 64, 5, 67, 24, 8, 62, 25, 40, 39, 25, 65, 33, 65, 15, 15, 5, 0, 75, 77, 71, 24, 65, 16, 71, 13, 44, 71, 72, 24, 39, 13, 35, 11, 41, 24, 8, 71, 32, 0, 77, 42, 24, 68, 52, 53, 2, 13, 32, 42, 60, 24, 7, 71, 46, 24, 46, 54, 13, 46, 3, 77, 24, 23, 54, 13, 46, 24, 13, 16, 16, 71, 13, 72, 77, 24, 46, 0, 24, 54, 13, 70, 71, 24, 54, 13, 16, 16, 71, 35, 71, 74, 24, 46, 0, 24, 46, 54, 71, 24, 23, 0, 24, 62, 75, 77, 77, 42, 13, 35, 24, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 48, 32, 13, 74, 42, 31, 42, 72, 24, 8, 75, 46, 42, 35, 24, 77, 16, 71, 13, 44, 77, 24, 13, 46, 24, 13, 24, 77, 75, 31, 31, 42, 24, 26, 49, 77, 24, 41, 0, 75, 24, 11, 13, 35, 24, 77, 71, 71, 57, 24, 31, 41, 24, 46, 23, 71, 71, 46, 77, 24, 13, 72, 71, 24, 16, 72, 71, 46, 46, 41, 24, 16, 13, 77, 77, 42, 0, 35, 13, 46, 71, 57, 17, 24, 64, 54, 71, 72, 71, 24, 54, 13, 77, 24, 34, 71, 71, 35, 24, 13, 24, 77, 46, 71, 13, 74, 41, 24, 74, 72, 75, 31, 34, 71, 13, 46, 24, 0, 60, 24, 13, 46, 46, 71, 35, 46, 42, 0, 35, 24, 60, 0, 72, 24, 24, 64, 54, 71, 24, 13, 34, 72, 75, 16, 46, 24, 23, 42, 46, 54, 74, 72, 13, 23, 13, 32, 24, 0, 60, 24, 10, 22, 65, 22, 24, 46, 72, 0, 0, 16, 77, 24, 60, 72, 0, 31, 24, 65, 41, 72, 42, 13, 24, 54, 13, 24, 15, 15, 18, 0, 75, 72, 35, 71, 72, 77, 24, 13, 46, 46, 71, 35, 74, 24, 13, 24, 60, 75, 35, 71, 72, 13, 32, 24, 60, 0, 72, 24, 37, 75, 72, 74, 42, 77, 54, 24, 16, 0, 32, 42, 46, 42, 11, 13, 32, 24, 24, 15, 15, 49, 35, 24, 71, 35, 36, 72, 13, 70, 42, 35, 36, 24, 34, 41, 24, 64, 54, 71, 0, 74, 0, 72, 24, 74, 71, 24, 66, 72, 41, 24, 74, 71, 16, 42, 11, 46, 42, 35, 36, 24, 2, 54, 72, 42, 77, 46, 0, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 23, 13, 32, 44, 77, 24, 46, 0, 24, 34, 0, 13, 72, 74, 24, 18, 13, 72, 42, 35, 71, 24, 25, 35, 71, 24, 0, 35, 24, 46, 54, 71, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 77, 46, 0, 16, 77, 24, 46, 0, 24, 46, 13, 32, 44, 24, 46, 0, 24, 31, 71, 31, 34, 71, 72, 77, 24, 0, 60, 24, 46, 54, 71, 24, 24, 49, 46, 24, 46, 54, 71, 24, 11, 71, 35, 46, 71, 72, 24, 0, 60, 24, 46, 54, 71, 24, 34, 32, 0, 0, 74, 41, 24, 72, 13, 31, 16, 13, 36, 71, 24, 75, 35, 60, 0, 32, 74, 42, 35, 36, 24, 42, 35, 24, 46, 24, 26, 64, 54, 71, 24, 43, 75, 71, 71, 35, 3, 77, 24, 65, 16, 71, 71, 11, 54, 17, 24, 42, 77, 24, 74, 71, 77, 42, 36, 35, 71, 74, 24, 46, 0, 24, 13, 11, 44, 35, 0, 23, 32, 71, 74, 36, 71, 24, 46, 54, 24, 15, 15, 66, 72, 0, 46, 54, 71, 72, 77, 24, 49, 35, 74, 72, 71, 41, 57, 24, 59, 57, 24, 65, 71, 72, 36, 71, 41, 57, 24, 1, 57, 24, 33, 32, 41, 13, 57, 24, 45, 57, 24, 13, 35, 74, 24, 65, 71, 72, 36, 24, 56, 42, 44, 71, 24, 13, 35, 24, 13, 36, 42, 35, 36, 24, 72, 0, 11, 44, 24, 77, 46, 13, 72, 57, 24, 46, 54, 71, 24, 16, 72, 71, 77, 42, 74, 71, 35, 46, 24, 42, 77, 24, 35, 0, 23, 24, 72, 71, 16, 72, 24, 69, 0, 72, 24, 6, 76, 24, 41, 71, 13, 72, 77, 57, 24, 46, 54, 71, 24, 29, 13, 11, 44, 71, 46, 24, 54, 75, 35, 36, 24, 42, 35, 24, 49, 35, 42, 46, 13, 24, 5, 71, 35, 74, 71, 72, 77, 0, 35, 3, 77, 24, 15, 15, 64, 54, 71, 24, 13, 75, 46, 54, 0, 72, 24, 49, 32, 42, 24, 30, 0, 35, 36, 22, 24, 68, 65, 46, 71, 16, 54, 13, 35, 42, 71, 24, 40, 0, 35, 0, 46, 38, 15, 15, 33, 60, 24, 49, 32, 42, 24, 30, 24, 15, 15, 8, 13, 75, 32, 24, 69, 13, 72, 72, 71, 32, 32, 57, 24, 13, 24, 30, 71, 77, 46, 24, 48, 42, 72, 36, 42, 35, 42, 13, 24, 13, 46, 46, 0, 72, 35, 71, 41, 24, 74, 72, 42, 70, 42, 35, 36, 24, 13, 24, 30, 71, 24, 54, 13, 70, 71, 24, 35, 0, 24, 42, 74, 71, 13, 24, 42, 60, 24, 46, 54, 71, 24, 13, 35, 0, 35, 41, 31, 0, 75, 77, 24, 0, 16, 53, 71, 74, 24, 23, 72, 42, 46, 71, 72, 24, 42, 77, 24, 77, 24, 64, 54, 71, 24, 0, 60, 60, 42, 11, 71, 72, 24, 23, 13, 77, 24, 46, 13, 44, 71, 35, 24, 46, 0, 24, 13, 24, 54, 0, 77, 16, 42, 46, 13, 32, 57, 24, 16, 0, 32, 42, 11, 71, 24, 77, 13, 42, 74, 57, 24, 24, 2, 0, 32, 75, 31, 35, 42, 77, 46, 15, 15, 30, 54, 13, 46, 24, 42, 77, 24, 42, 46, 24, 13, 34, 0, 75, 46, 24, 46, 42, 16, 16, 42, 35, 36, 24, 46, 54, 13, 46, 24, 31, 13, 44, 71, 77, 24, 16, 71, 0, 24, 66, 13, 35, 44, 77, 24, 69, 71, 74, 71, 72, 13, 32, 24, 36, 0, 70, 71, 72, 35, 31, 71, 35, 46, 24, 0, 60, 60, 42, 11, 71, 77, 24, 8, 0, 77, 46, 24, 0, 60, 60, 42, 11, 71, 77, 24, 2, 0, 75, 72, 24, 15, 15, 15, 15, 8, 75, 34, 32, 42, 77, 54, 71, 74, 24, 65, 71, 16, 46, 71, 31, 34, 71, 72, 24, 6, 1, 57, 24, 19, 76, 6, 1, 15, 15, 25, 35, 32, 42, 35, 71, 24, 13, 74, 70, 71, 72, 46, 42, 77, 42, 24, 5, 71, 24, 77, 54, 42, 16, 16, 71, 74, 24, 0, 75, 46, 24, 42, 35, 24, 46, 54, 71, 24, 77, 16, 72, 42, 35, 36, 24, 0, 60, 24, 6, 73, 12, 12, 24, 60, 0, 72, 24, 67, 75, 72, 0, 16, 71, 57, 24, 31, 24, 26, 67, 70, 71, 35, 46, 75, 13, 32, 32, 41, 24, 33, 24, 23, 0, 44, 71, 24, 75, 16, 24, 13, 35, 74, 24, 32, 0, 0, 44, 71, 74, 24, 13, 46, 24, 46, 54, 71, 24, 77, 11, 0, 72, 71, 24, 13, 35, 74, 24, 24, 30, 54, 71, 35, 24, 33, 24, 54, 13, 74, 24, 13, 36, 72, 71, 71, 74, 24, 46, 0, 24, 60, 0, 77, 46, 71, 72, 24, 46, 54, 42, 77, 24, 54, 0, 31, 71, 32, 71, 77, 77, 24, 11, 13, 35, 42, 35, 71, 24, 60, 24, 64, 54, 71, 24, 67, 35, 36, 32, 42, 77, 54, 24, 36, 13, 72, 74, 71, 35, 71, 72, 24, 13, 35, 74, 24, 16, 0, 71, 46, 24, 18, 13, 72, 11, 24, 5, 13, 31, 71, 72, 24, 77, 16, 71, 35, 46, 24, 31, 75, 11, 24, 15, 15, 18, 0, 75, 72, 35, 71, 72, 77, 24, 13, 46, 46, 71, 35, 74, 24, 13, 24, 60, 75, 35, 71, 72, 13, 32, 24, 60, 0, 72, 24, 37, 75, 72, 74, 42, 77, 54, 24, 16, 0, 32, 42, 46, 42, 11, 13, 32, 24, 24, 15, 15, 62, 75, 77, 77, 42, 13, 35, 24, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 48, 32, 13, 74, 42, 31, 42, 72, 24, 8, 75, 46, 42, 35, 24, 13, 35, 74, 24, 65, 13, 75, 74, 42, 24, 49, 72, 13, 34, 42, 24, 15, 15, 33, 35, 24, 39, 0, 70, 71, 31, 34, 71, 72, 24, 19, 76, 76, 19, 57, 24, 56, 71, 71, 24, 66, 0, 41, 74, 24, 18, 13, 32, 70, 0, 57, 24, 6, 45, 57, 24, 32, 71, 13, 70, 71, 77, 24, 46, 54, 71, 24, 52, 71, 77, 16, 42, 46, 71, 24, 54, 42, 77, 24, 60, 72, 75, 77, 46, 72, 13, 46, 42, 0, 35, 57, 24, 46, 54, 71, 24, 71, 70, 71, 72, 53, 11, 13, 32, 31, 24, 18, 11, 56, 13, 75, 72, 42, 35, 24, 77, 0, 24, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 3, 77, 24, 0, 72, 74, 71, 72, 24, 46, 0, 24, 23, 42, 46, 54, 74, 72, 13, 23, 24, 71, 77, 77, 71, 35, 46, 42, 13, 32, 32, 41, 24, 13, 32, 24, 49, 77, 24, 64, 72, 75, 31, 16, 3, 77, 24, 77, 71, 35, 42, 0, 72, 24, 30, 54, 42, 46, 71, 24, 5, 0, 75, 77, 71, 24, 13, 74, 70, 42, 77, 71, 72, 24, 0, 35, 24, 62, 75, 77, 77, 42, 13, 24, 13, 35, 24, 64, 54, 42, 77, 24, 46, 42, 31, 71, 57, 24, 16, 32, 13, 41, 71, 72, 77, 24, 23, 54, 0, 24, 32, 0, 36, 36, 71, 74, 24, 42, 35, 24, 13, 46, 24, 19, 24, 16, 22, 31, 22, 24, 67, 13, 77, 46, 71, 72, 35, 24, 49, 74, 70, 42, 11, 71, 24, 11, 0, 32, 75, 31, 35, 42, 77, 46, 15, 15, 49, 74, 13, 16, 46, 71, 74, 24, 60, 72, 0, 31, 24, 13, 35, 24, 0, 35, 32, 42, 35, 71, 24, 74, 42, 77, 11, 75, 77, 77, 42, 0, 24, 49, 46, 24, 46, 54, 71, 24, 11, 71, 35, 46, 71, 72, 24, 0, 60, 24, 46, 54, 71, 24, 34, 32, 0, 0, 74, 41, 24, 72, 13, 31, 16, 13, 36, 71, 24, 75, 35, 60, 0, 32, 74, 42, 35, 36, 24, 42, 35, 24, 46, 24, 15, 15, 69, 72, 71, 35, 11, 54, 24, 69, 0, 72, 71, 42, 36, 35, 24, 18, 42, 35, 42, 77, 46, 71, 72, 24, 28, 71, 13, 35, 53, 7, 70, 71, 77, 24, 56, 71, 24, 52, 72, 42, 13, 35, 24, 77, 16, 71, 13, 44, 24, 64, 54, 71, 13, 46, 71, 72, 24, 11, 72, 42, 46, 42, 11, 15, 15, 64, 0, 24, 34, 71, 24, 23, 71, 32, 11, 0, 31, 71, 74, 57, 24, 72, 71, 77, 16, 71, 11, 46, 60, 75, 32, 32, 41, 57, 24, 42, 35, 46, 0, 24, 39, 13, 35, 11, 41, 24, 49, 31, 0, 35, 77, 57, 24, 13, 24, 32, 0, 35, 36, 46, 42, 31, 71, 24, 42, 35, 70, 71, 77, 46, 42, 36, 13, 46, 42, 70, 71, 24, 72, 71, 16, 0, 72, 46, 71, 72, 24, 13, 46, 24, 24, 15, 15, 49, 75, 46, 54, 0, 72, 42, 46, 42, 71, 77, 24, 72, 71, 77, 16, 0, 35, 74, 24, 46, 0, 24, 64, 0, 23, 35, 24, 2, 71, 35, 46, 71, 72, 24, 13, 46, 24, 66, 0, 11, 13, 24, 62, 13, 46, 0, 35, 24, 10, 16, 74, 13, 46, 71, 74, 24, 28, 75, 35, 71, 24, 12, 57, 24, 19, 76, 6, 59, 15, 15, 64, 54, 42, 77, 24, 64, 71, 72, 31, 77, 24, 0, 60, 24, 65, 13, 32, 71, 24, 36, 0, 70, 71, 72, 35, 77, 24, 46, 24, 30, 71, 24, 13, 72, 71, 24, 13, 32, 32, 24, 60, 75, 32, 32, 24, 0, 60, 24, 31, 75, 11, 75, 77, 57, 24, 13, 35, 74, 24, 35, 0, 46, 24, 0, 35, 32, 41, 24, 34, 71, 11, 13, 75, 77, 71, 24, 11, 0, 32, 24, 69, 0, 72, 24, 16, 71, 0, 16, 32, 71, 24, 23, 54, 0, 24, 72, 71, 36, 75, 32, 13, 72, 32, 41, 24, 23, 0, 72, 44, 24, 32, 0, 35, 36, 24, 54, 0, 75, 72, 77, 24, 55, 24, 74, 71, 60, 42, 35, 71, 74, 24, 64, 54, 71, 24, 46, 54, 72, 71, 13, 46, 24, 46, 0, 24, 54, 13, 72, 74, 53, 23, 0, 35, 24, 23, 0, 31, 71, 35, 3, 77, 24, 72, 42, 36, 54, 46, 77, 24, 42, 35, 24, 62, 0, 29, 13, 70, 13, 24, 42, 77, 24, 15, 15, 62, 71, 32, 13, 46, 42, 70, 71, 77, 24, 0, 35, 24, 65, 13, 46, 75, 72, 74, 13, 41, 24, 31, 0, 75, 72, 35, 24, 13, 46, 24, 46, 54, 71, 24, 36, 72, 13, 70, 71, 24, 0, 60, 24, 5, 13, 32, 42, 24, 66, 75, 46, 24, 75, 35, 74, 42, 77, 11, 32, 0, 77, 71, 74, 24, 42, 35, 24, 46, 54, 71, 24, 77, 46, 75, 74, 41, 24, 23, 13, 77, 24, 46, 54, 13, 46, 24, 39, 75, 46, 72, 42, 62, 67, 2, 65, 57, 24, 13, 24, 49, 16, 16, 72, 0, 13, 11, 54, 42, 35, 36, 24, 42, 46, 77, 24, 6, 20, 76, 46, 54, 24, 34, 42, 72, 46, 54, 74, 13, 41, 24, 39, 0, 70, 22, 24, 1, 57, 24, 11, 0, 32, 32, 71, 36, 71, 24, 60, 0, 0, 24, 15, 15, 26, 52, 72, 71, 13, 31, 24, 52, 0, 0, 72, 23, 13, 41, 77, 17, 24, 60, 72, 0, 31, 24, 62, 50, 66, 27, 16, 0, 16, 24, 77, 46, 13, 72, 24, 13, 35, 74, 24, 49, 32, 71, 58, 13, 35, 74, 72, 42, 24, 39, 13, 46, 75, 72, 71, 24, 46, 71, 13, 77, 71, 77, 24, 75, 77, 24, 46, 0, 74, 13, 41, 24, 23, 42, 46, 54, 24, 77, 0, 31, 71, 24, 60, 32, 42, 72, 46, 13, 46, 42, 0, 75, 77, 24, 77, 54, 0, 23, 71, 24, 8, 75, 34, 32, 42, 77, 54, 71, 74, 4, 24, 18, 13, 41, 24, 19, 12, 57, 24, 19, 76, 6, 59, 15, 15, 30, 8, 24, 2, 0, 31, 16, 13, 35, 41, 24, 56, 56, 2, 24, 68, 26, 64, 54, 71, 24, 30, 13, 77, 54, 24, 15, 15, 49, 35, 24, 75, 35, 74, 13, 46, 71, 74, 24, 60, 42, 32, 71, 24, 16, 54, 0, 46, 0, 24, 0, 60, 24, 65, 0, 75, 46, 54, 24, 37, 0, 72, 71, 13, 35, 24, 77, 42, 35, 36, 71, 72, 24, 13, 35, 74, 24, 17, 66, 71, 46, 23, 71, 71, 35, 24, 46, 54, 71, 24, 64, 0, 23, 71, 72, 77, 57, 17, 24, 77, 54, 0, 23, 35, 24, 54, 71, 72, 71, 57, 24, 42, 77, 24, 46, 54, 71, 24, 11, 71, 35, 46, 72, 13, 32, 24, 42, 24, 49, 46, 24, 46, 54, 71, 24, 11, 71, 35, 46, 71, 72, 24, 0, 60, 24, 2, 13, 16, 42, 46, 13, 32, 24, 25, 35, 71, 24, 49, 72, 71, 35, 13, 57, 24, 16, 13, 11, 44, 71, 74, 24, 23, 42, 46, 54, 24, 31, 0, 24, 15, 15, 65, 71, 72, 70, 71, 72, 24, 63, 0, 71, 24, 30, 13, 32, 16, 0, 32, 71, 24, 16, 72, 71, 77, 71, 35, 46, 77, 24, 46, 54, 71, 24, 26, 60, 42, 32, 32, 71, 46, 24, 0, 60, 24, 60, 42, 77, 54, 17, 24, 66, 75, 46, 24, 54, 71, 24, 23, 54, 0, 24, 42, 77, 24, 35, 0, 46, 24, 23, 42, 46, 54, 0, 75, 46, 24, 77, 42, 35, 24, 77, 54, 0, 75, 32, 74, 24, 35, 0, 46, 24, 32, 42, 70, 71, 24, 42, 35, 24, 13, 24, 8, 75, 34, 32, 42, 77, 54, 71, 74, 4, 24, 28, 75, 32, 41, 24, 6, 57, 24, 19, 76, 6, 12, 22, 15, 15, 64, 54, 71, 77, 71, 24, 64, 71, 72, 31, 77, 24, 0, 60, 24, 65, 71, 72, 70, 42, 11, 71, 24, 68, 24, 62, 0, 34, 71, 72, 46, 24, 69, 0, 72, 77, 46, 71, 72, 57, 24, 13, 24, 34, 72, 0, 0, 74, 42, 35, 36, 57, 24, 72, 75, 36, 36, 71, 74, 32, 41, 24, 54, 13, 35, 74, 77, 0, 31, 71, 24, 5, 0, 32, 32, 24, 64, 54, 71, 24, 70, 42, 13, 34, 32, 71, 24, 60, 13, 35, 46, 13, 77, 41, 24, 74, 71, 60, 71, 35, 77, 71, 77, 24, 0, 60, 24, 46, 54, 71, 24, 65, 46, 71, 71, 32, 71, 72, 77, 57, 24, 66, 75, 11, 11, 13, 24, 30, 71, 24, 42, 35, 70, 42, 46, 71, 24, 41, 0, 75, 24, 46, 0, 24, 77, 75, 34, 31, 42, 46, 24, 41, 0, 75, 72, 24, 70, 42, 74, 71, 0, 77, 57, 24, 16, 54, 0, 46, 0, 77, 57, 24, 13, 72, 46, 42, 11, 24, 56, 71, 2, 72, 0, 41, 24, 54, 13, 74, 24, 23, 13, 42, 46, 71, 74, 24, 46, 0, 24, 74, 0, 24, 46, 54, 42, 77, 22, 24, 5, 71, 24, 77, 16, 71, 35, 46, 24, 46, 54, 71, 24, 71, 13, 72, 32, 41, 24, 74, 24, 26, 8, 54, 42, 32, 32, 42, 16, 77, 24, 31, 13, 41, 24, 23, 13, 35, 46, 24, 75, 77, 24, 46, 0, 24, 46, 54, 42, 35, 44, 24, 54, 71, 3, 77, 24, 36, 42, 70, 42, 35, 36, 24, 75, 77, 24, 13, 24, 31, 0, 24, 64, 54, 71, 77, 71, 24, 13, 60, 46, 71, 72, 35, 0, 0, 35, 77, 24, 42, 35, 24, 18, 71, 72, 42, 74, 42, 13, 35, 24, 5, 42, 32, 32, 24, 8, 13, 72, 44, 24, 13, 72, 71, 24, 72, 71, 16, 72, 71, 77, 71, 24, 25, 35, 24, 25, 11, 46, 22, 24, 19, 6, 57, 24, 13, 60, 46, 71, 72, 24, 13, 35, 24, 13, 32, 31, 0, 77, 46, 24, 60, 42, 70, 71, 53, 31, 0, 35, 46, 54, 24, 11, 32, 0, 77, 75, 72, 71, 57, 24, 46, 54, 24, 39, 13, 46, 75, 72, 71, 24, 46, 71, 13, 77, 71, 77, 24, 75, 77, 24, 46, 0, 74, 13, 41, 24, 23, 42, 46, 54, 24, 77, 0, 31, 71, 24, 60, 32, 42, 72, 46, 13, 46, 42, 0, 75, 77, 24, 77, 54, 0, 23, 71, 24, 40, 71, 35, 71, 72, 13, 32, 32, 41, 57, 24, 46, 54, 71, 24, 11, 75, 32, 16, 72, 42, 46, 77, 24, 13, 72, 71, 24, 46, 54, 0, 77, 71, 24, 46, 72, 41, 42, 35, 36, 24, 46, 0, 24, 11, 75, 46, 24, 11, 0, 24, 49, 24, 60, 71, 23, 24, 60, 13, 11, 46, 0, 72, 77, 24, 11, 0, 35, 46, 72, 42, 34, 75, 46, 71, 74, 24, 46, 0, 24, 46, 54, 42, 77, 24, 16, 32, 13, 11, 42, 74, 24, 13, 11, 11, 71, 16, 46, 13, 35, 11, 24, 28, 75, 32, 42, 13, 35, 13, 24, 40, 0, 35, 61, 14, 32, 71, 61, 24, 13, 35, 74, 24, 65, 13, 32, 42, 31, 24, 2, 54, 75, 29, 60, 42, 15, 15, 6, 73, 24, 41, 71, 13, 72, 77, 24, 42, 35, 24, 66, 71, 72, 24, 30, 13, 35, 46, 24, 77, 31, 13, 72, 46, 24, 13, 35, 13, 32, 41, 77, 42, 77, 24, 0, 60, 24, 46, 54, 71, 24, 31, 0, 77, 46, 24, 42, 31, 16, 0, 72, 46, 13, 35, 46, 24, 35, 71, 23, 77, 24, 42, 35, 24, 24, 15, 15, 64, 71, 72, 72, 41, 24, 18, 11, 56, 13, 75, 72, 42, 35, 24, 31, 13, 44, 71, 77, 24, 13, 24, 46, 0, 75, 11, 54, 74, 0, 23, 35, 24, 11, 13, 46, 11, 54, 24, 42, 35, 24, 46, 54, 71, 24, 46, 54, 24, 25, 70, 71, 72, 70, 42, 71, 23, 15, 15, 64, 23, 0, 24, 46, 54, 42, 35, 36, 77, 24, 77, 46, 0, 16, 24, 31, 71, 24, 60, 72, 0, 31, 24, 31, 13, 44, 42, 35, 36, 24, 16, 0, 46, 24, 16, 42, 71, 77, 24, 24, 67, 70, 71, 35, 24, 75, 35, 74, 71, 72, 24, 42, 74, 71, 13, 32, 24, 11, 0, 35, 74, 42, 46, 42, 0, 35, 77, 57, 24, 34, 13, 44, 42, 35, 36, 24, 11, 13, 35, 24, 34, 71, 24, 13, 24, 34, 42, 46, 24, 0, 24, 52, 75, 46, 11, 54, 71, 77, 77, 24, 46, 54, 71, 24, 60, 0, 58, 24, 46, 71, 72, 72, 42, 71, 72, 24, 77, 21, 75, 71, 71, 61, 71, 74, 24, 0, 75, 46, 24, 46, 54, 71, 24, 74, 0, 0, 72, 24, 13, 46, 24, 24, 10, 22, 65, 22, 24, 11, 0, 72, 16, 0, 72, 13, 46, 71, 24, 16, 72, 0, 60, 42, 46, 77, 24, 13, 72, 71, 24, 77, 46, 13, 36, 35, 13, 46, 42, 35, 36, 24, 74, 75, 71, 24, 46, 0, 24, 46, 54, 71, 24, 46, 24, 64, 54, 71, 24, 46, 54, 72, 71, 13, 46, 24, 46, 0, 24, 54, 13, 72, 74, 53, 23, 0, 35, 24, 23, 0, 31, 71, 35, 3, 77, 24, 72, 42, 36, 54, 46, 77, 24, 42, 35, 24, 62, 0, 29, 13, 70, 13, 24, 42, 77, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 77, 16, 71, 13, 44, 77, 24, 46, 0, 24, 31, 71, 31, 34, 71, 72, 77, 24, 0, 60, 24, 46, 54, 71, 24, 31, 71, 74, 42, 13, 24, 13, 24, 25, 35, 24, 64, 54, 75, 72, 77, 74, 13, 41, 57, 24, 60, 0, 75, 72, 24, 74, 13, 41, 77, 24, 13, 60, 46, 71, 72, 24, 54, 71, 24, 77, 71, 46, 24, 0, 75, 46, 24, 0, 35, 24, 54, 42, 77, 24, 46, 72, 71, 24, 15, 15, 66, 32, 75, 71, 24, 77, 44, 42, 71, 77, 24, 0, 35, 24, 65, 23, 13, 35, 35, 24, 65, 46, 72, 71, 71, 46, 24, 39, 30, 24, 42, 35, 24, 46, 54, 71, 24, 52, 42, 77, 46, 72, 42, 11, 46, 24, 32, 13, 24, 69, 0, 72, 24, 71, 58, 13, 31, 16, 32, 71, 57, 24, 13, 31, 0, 35, 36, 24, 46, 54, 71, 24, 77, 42, 58, 24, 72, 71, 77, 42, 74, 71, 35, 11, 71, 77, 24, 42, 35, 24, 46, 54, 71, 24, 35, 71, 23, 24, 11, 24, 18, 75, 11, 54, 24, 0, 60, 24, 2, 54, 42, 35, 13, 3, 77, 24, 11, 54, 13, 16, 46, 71, 72, 77, 24, 0, 70, 71, 72, 24, 46, 54, 71, 24, 16, 13, 77, 46, 24, 77, 71, 70, 71, 35, 24, 74, 71, 11, 13, 74, 24, 30, 54, 71, 35, 24, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 13, 35, 35, 0, 75, 35, 11, 71, 74, 24, 54, 42, 77, 24, 74, 71, 11, 42, 77, 42, 0, 35, 24, 46, 0, 24, 16, 75, 32, 24, 66, 75, 46, 24, 46, 54, 13, 35, 44, 77, 24, 46, 0, 24, 52, 71, 70, 32, 42, 35, 24, 5, 0, 74, 36, 71, 77, 57, 24, 52, 71, 70, 42, 35, 24, 66, 75, 77, 54, 24, 13, 35, 74, 24, 28, 13, 31, 71, 77, 24, 24, 8, 75, 34, 32, 42, 77, 54, 71, 74, 4, 24, 28, 75, 32, 41, 24, 6, 57, 24, 19, 76, 6, 12, 22, 15, 15, 64, 54, 71, 77, 71, 24, 64, 71, 72, 31, 77, 24, 0, 60, 24, 65, 71, 72, 70, 42, 11, 71, 24, 68, 24, 69, 72, 0, 31, 24, 0, 35, 71, 24, 16, 71, 72, 77, 16, 71, 11, 46, 42, 70, 71, 57, 24, 46, 54, 42, 77, 24, 72, 71, 35, 46, 13, 32, 24, 16, 72, 0, 16, 71, 72, 46, 41, 24, 42, 77, 24, 0, 23, 35, 71, 24, 64, 54, 72, 0, 75, 36, 54, 0, 75, 46, 24, 31, 41, 24, 32, 42, 60, 71, 57, 24, 33, 3, 70, 71, 24, 34, 71, 71, 35, 24, 46, 71, 13, 77, 71, 74, 24, 34, 41, 24, 0, 46, 54, 71, 72, 24, 23, 0, 31, 71, 24, 64, 54, 71, 24, 16, 72, 0, 34, 32, 71, 31, 24, 0, 60, 24, 77, 32, 71, 71, 16, 42, 35, 36, 24, 42, 35, 24, 44, 35, 0, 23, 77, 24, 35, 0, 24, 11, 75, 32, 46, 75, 72, 13, 32, 57, 24, 36, 71, 0, 36, 24, 15, 15, 65, 64, 25, 2, 37, 24, 33, 18, 49, 40, 67, 4, 24, 18, 71, 74, 42, 11, 13, 72, 71, 24, 13, 16, 16, 32, 42, 11, 13, 46, 42, 0, 35, 24, 60, 0, 72, 31, 24, 23, 42, 46, 54, 24, 77, 46, 71, 46, 24, 64, 13, 32, 44, 42, 35, 36, 24, 46, 0, 24, 46, 54, 71, 24, 31, 71, 74, 42, 13, 24, 13, 34, 0, 75, 46, 24, 13, 35, 24, 54, 0, 75, 72, 24, 13, 60, 46, 71, 72, 24, 72, 71, 11, 71, 42, 70, 42, 35, 36, 24, 49, 32, 71, 58, 71, 42, 24, 56, 71, 0, 35, 0, 70, 57, 24, 13, 24, 65, 0, 70, 42, 71, 46, 24, 11, 0, 77, 31, 0, 35, 13, 75, 46, 24, 23, 54, 0, 24, 42, 35, 24, 6, 73, 1, 20, 24, 34, 71, 11, 13, 24, 64, 23, 0, 24, 11, 71, 72, 46, 13, 42, 35, 46, 42, 71, 77, 24, 13, 34, 0, 75, 46, 24, 46, 54, 71, 24, 39, 69, 56, 24, 13, 72, 71, 24, 46, 54, 13, 46, 24, 35, 0, 46, 54, 42, 35, 36, 24, 77, 46, 13, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 77, 16, 0, 44, 71, 24, 13, 46, 24, 46, 54, 71, 24, 48, 13, 32, 75, 71, 77, 24, 48, 0, 46, 71, 72, 24, 65, 75, 31, 31, 42, 46, 24, 64, 54, 71, 24, 16, 32, 13, 35, 57, 24, 23, 54, 42, 11, 54, 24, 23, 0, 75, 32, 74, 24, 16, 72, 0, 34, 13, 34, 32, 41, 24, 60, 13, 11, 71, 24, 77, 75, 34, 77, 46, 13, 35, 46, 42, 13, 32, 24, 16, 75, 24, 10, 16, 74, 13, 46, 71, 74, 24, 28, 13, 35, 75, 13, 72, 41, 24, 73, 57, 24, 19, 76, 6, 45, 15, 15, 64, 54, 42, 77, 24, 64, 71, 72, 31, 77, 24, 0, 60, 24, 65, 13, 32, 71, 24, 36, 0, 70, 71, 72, 35, 24, 15, 15, 49, 32, 42, 24, 62, 71, 61, 13, 24, 49, 46, 13, 42, 71, 24, 13, 46, 24, 54, 42, 77, 24, 54, 0, 31, 71, 24, 42, 35, 24, 40, 72, 13, 35, 74, 24, 8, 72, 13, 42, 72, 42, 71, 57, 24, 64, 71, 58, 24, 30, 71, 32, 11, 0, 31, 71, 24, 46, 0, 24, 23, 71, 71, 44, 24, 60, 0, 75, 72, 24, 0, 60, 24, 46, 54, 71, 24, 42, 31, 16, 71, 13, 11, 54, 31, 71, 35, 46, 24, 42, 35, 21, 75, 42, 72, 41, 22, 24, 65, 24, 33, 3, 31, 24, 35, 0, 46, 24, 77, 75, 72, 71, 24, 23, 54, 71, 46, 54, 71, 72, 24, 42, 46, 24, 21, 75, 13, 32, 42, 60, 42, 71, 77, 24, 13, 77, 24, 11, 0, 31, 60, 0, 72, 46, 42, 35, 36, 24, 46, 0, 24, 64, 54, 42, 77, 24, 46, 42, 31, 71, 57, 24, 16, 32, 13, 41, 71, 72, 77, 24, 23, 54, 0, 24, 32, 0, 36, 36, 71, 74, 24, 42, 35, 24, 13, 46, 24, 19, 24, 16, 22, 31, 22, 24, 67, 13, 77, 46, 71, 72, 35, 24, 33, 46, 24, 23, 13, 77, 35, 3, 46, 24, 29, 75, 77, 46, 24, 46, 23, 0, 24, 71, 58, 11, 71, 16, 46, 42, 0, 35, 13, 32, 24, 46, 71, 13, 31, 77, 24, 13, 35, 74, 24, 46, 23, 0, 24, 77, 75, 11, 11, 71, 24, 49, 24, 23, 42, 46, 35, 71, 77, 77, 24, 77, 13, 42, 74, 24, 46, 54, 13, 46, 24, 46, 54, 71, 24, 70, 71, 54, 42, 11, 32, 71, 24, 42, 35, 70, 0, 32, 70, 71, 74, 24, 31, 13, 41, 24, 54, 13, 70, 71, 24, 24, 9, 65, 16, 71, 13, 44, 42, 35, 36, 24, 75, 16, 24, 42, 77, 24, 13, 32, 23, 13, 41, 77, 24, 46, 54, 71, 24, 72, 42, 36, 54, 46, 24, 46, 54, 42, 35, 36, 24, 46, 0, 24, 74, 0, 9, 4, 24, 25, 32, 41, 24, 65, 71, 70, 71, 35, 24, 0, 60, 24, 32, 13, 77, 46, 24, 41, 71, 13, 72, 3, 77, 24, 16, 0, 77, 46, 77, 71, 13, 77, 0, 35, 24, 46, 71, 13, 31, 77, 24, 13, 16, 16, 71, 13, 72, 24, 46, 0, 24, 34, 71, 24, 15, 15, 49, 24, 43, 75, 42, 11, 54, 71, 24, 42, 35, 74, 42, 36, 71, 35, 0, 75, 77, 24, 23, 0, 31, 13, 35, 24, 77, 46, 13, 35, 74, 77, 24, 60, 13, 11, 42, 35, 36, 24, 13, 24, 31, 0, 35, 75, 31, 71, 24, 2, 46, 72, 32, 24, 47, 24, 39, 15, 15, 49, 42, 72, 34, 35, 34, 24, 11, 0, 53, 60, 0, 75, 35, 74, 71, 72, 24, 13, 35, 74, 24, 2, 67, 25, 24, 66, 72, 42, 13, 35, 24, 2, 54, 71, 77, 44, 41, 22, 24, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 77, 54, 13, 44, 71, 77, 24, 54, 13, 35, 74, 77, 24, 23, 42, 46, 54, 24, 2, 54, 42, 35, 71, 77, 71, 24, 48, 42, 11, 71, 24, 8, 24, 2, 0, 35, 46, 72, 42, 34, 75, 46, 42, 35, 36, 24, 11, 0, 32, 75, 31, 35, 42, 77, 46, 15, 15, 64, 54, 71, 24, 5, 0, 75, 77, 71, 24, 42, 77, 24, 0, 35, 24, 60, 42, 72, 71, 22, 24, 49, 35, 74, 24, 24, 30, 54, 71, 35, 24, 0, 46, 54, 71, 72, 24, 42, 35, 31, 13, 46, 71, 77, 24, 77, 13, 23, 24, 60, 0, 75, 72, 24, 36, 75, 13, 72, 74, 77, 24, 34, 71, 13, 46, 42, 35, 36, 24, 13, 24, 54, 13, 35, 74, 11, 24, 15, 15, 28, 75, 32, 42, 71, 24, 49, 35, 74, 72, 71, 23, 77, 24, 42, 35, 24, 26, 64, 54, 71, 24, 65, 0, 75, 35, 74, 24, 0, 60, 24, 18, 75, 77, 42, 11, 22, 17, 24, 68, 49, 32, 32, 77, 46, 13, 72, 24, 24, 2, 54, 72, 0, 35, 42, 11, 24, 16, 13, 42, 35, 24, 42, 77, 24, 13, 24, 23, 42, 74, 71, 77, 16, 72, 71, 13, 74, 24, 16, 72, 0, 34, 32, 71, 31, 24, 42, 35, 24, 46, 54, 71, 24, 10, 35, 42, 46, 71, 74, 24, 33, 35, 24, 26, 18, 42, 77, 46, 71, 72, 24, 49, 31, 71, 72, 42, 11, 13, 57, 17, 24, 13, 24, 31, 0, 11, 44, 75, 31, 71, 35, 46, 13, 72, 41, 24, 46, 54, 13, 46, 24, 74, 71, 34, 75, 46, 71, 74, 24, 32, 24, 52, 72, 42, 70, 71, 72, 77, 24, 77, 54, 0, 75, 32, 74, 24, 11, 0, 35, 77, 42, 74, 71, 72, 24, 13, 70, 0, 42, 74, 42, 35, 36, 24, 46, 54, 71, 24, 13, 72, 71, 13, 57, 24, 13, 35, 74, 24, 46, 54, 0, 24, 30, 71, 24, 35, 0, 46, 42, 11, 71, 74, 24, 41, 0, 75, 3, 72, 71, 24, 34, 32, 0, 11, 44, 42, 35, 36, 24, 13, 74, 77, 51, 15, 15, 37, 71, 71, 16, 24, 77, 75, 16, 16, 0, 72, 46, 42, 35, 36, 24, 36, 24, 26, 30, 71, 24, 23, 71, 72, 71, 24, 77, 42, 46, 46, 42, 35, 36, 24, 46, 54, 71, 72, 71, 24, 60, 0, 72, 24, 23, 54, 13, 46, 24, 60, 71, 32, 46, 24, 32, 42, 44, 71, 24, 60, 0, 72, 71, 70, 71, 72, 57, 24, 5, 71, 72, 71, 3, 77, 24, 46, 54, 71, 42, 72, 24, 13, 74, 70, 42, 11, 71, 24, 46, 0, 24, 75, 16, 36, 72, 13, 74, 71, 24, 41, 0, 75, 72, 24, 36, 13, 31, 71, 4, 15, 15, 6, 22, 24, 66, 71, 24, 21, 24, 26, 2, 13, 35, 24, 41, 0, 75, 24, 42, 31, 13, 36, 42, 35, 71, 24, 34, 71, 42, 35, 36, 24, 65, 0, 31, 13, 32, 42, 53, 49, 31, 71, 72, 42, 11, 13, 35, 24, 13, 35, 74, 24, 23, 13, 46, 11, 54, 42, 35, 24, 15, 15, 49, 35, 46, 42, 53, 36, 0, 70, 71, 72, 35, 31, 71, 35, 46, 24, 74, 71, 31, 0, 35, 77, 46, 72, 13, 46, 0, 72, 77, 24, 54, 0, 32, 74, 24, 10, 22, 65, 22, 24, 60, 32, 13, 36, 77, 24, 13, 77, 24, 64, 54, 71, 24, 11, 0, 75, 72, 46, 24, 42, 35, 77, 42, 77, 46, 71, 74, 24, 0, 35, 24, 75, 35, 13, 35, 42, 31, 0, 75, 77, 24, 0, 16, 42, 35, 42, 0, 35, 77, 57, 24, 60, 0, 72, 24, 42, 35, 77, 46, 24, 64, 71, 11, 54, 35, 42, 11, 13, 32, 32, 41, 57, 24, 25, 44, 46, 0, 34, 71, 72, 60, 71, 77, 46, 24, 54, 13, 77, 24, 11, 0, 31, 71, 24, 13, 35, 74, 24, 36, 0, 35, 71, 57, 24, 42, 60, 24, 23, 71, 3, 24, 25, 35, 71, 24, 34, 42, 36, 24, 72, 71, 13, 77, 0, 35, 24, 60, 0, 72, 24, 46, 54, 71, 24, 75, 16, 46, 42, 11, 44, 4, 24, 64, 54, 71, 24, 75, 16, 72, 0, 13, 72, 24, 0, 70, 71, 72, 24, 64, 72, 75, 24, 15, 15, 49, 24, 8, 13, 35, 71, 72, 13, 24, 66, 72, 71, 13, 74, 24, 77, 46, 0, 72, 71, 22, 24, 68, 62, 13, 60, 13, 71, 32, 24, 2, 72, 42, 77, 0, 77, 46, 0, 31, 0, 27, 69, 0, 72, 24, 64, 54, 71, 24, 15, 15, 8, 0, 16, 75, 32, 13, 72, 24, 46, 0, 75, 72, 42, 77, 46, 24, 13, 46, 46, 72, 13, 11, 46, 42, 0, 35, 24, 25, 32, 74, 24, 18, 13, 35, 9, 77, 24, 2, 13, 70, 71, 24, 42, 35, 24, 5, 0, 11, 24, 15, 15, 8, 72, 71, 77, 42, 74, 71, 35, 46, 24, 64, 72, 75, 31, 16, 24, 23, 13, 32, 44, 77, 24, 46, 0, 24, 34, 0, 13, 72, 74, 24, 18, 13, 72, 42, 35, 71, 24, 25, 35, 71, 24, 0, 35, 24, 46, 54, 71, 24, 56, 42, 44, 71, 24, 13, 35, 24, 13, 36, 42, 35, 36, 24, 72, 0, 11, 44, 24, 77, 46, 13, 72, 57, 24, 46, 54, 71, 24, 16, 72, 71, 77, 42, 74, 71, 35, 46, 24, 42, 77, 24, 35, 0, 23, 24, 72, 71, 16, 72, 24, 30, 71, 24, 71, 35, 11, 0, 75, 72, 13, 36, 71, 24, 35, 71, 23, 24, 13, 35, 74, 24, 42, 35, 35, 0, 70, 13, 46, 42, 70, 71, 24, 75, 77, 71, 77, 24, 0, 60, 24, 0, 75, 72, 24, 11, 0, 35, 46, 71, 35, 24, 5, 71, 24, 46, 0, 32, 74, 24, 54, 71, 72, 24, 54, 71, 3, 74, 24, 60, 75, 74, 36, 71, 74, 24, 13, 24, 60, 71, 23, 24, 46, 54, 42, 35, 36, 77, 24, 13, 46, 24, 54, 42, 77, 24, 46, 72, 13, 70, 71, 32, 24, 33, 35, 24, 31, 41, 24, 16, 72, 42, 70, 13, 46, 71, 24, 16, 72, 13, 11, 46, 42, 11, 71, 24, 13, 77, 24, 13, 24, 75, 72, 0, 32, 0, 36, 42, 77, 46, 57, 24, 33, 24, 72, 71, 11, 71, 35, 46, 32, 41, 24, 24, 15, 15, 49, 24, 32, 0, 77, 77, 24, 46, 0, 24, 46, 54, 71, 24, 28, 71, 46, 77, 24, 74, 42, 74, 24, 35, 0, 46, 24, 32, 71, 13, 70, 71, 24, 2, 0, 23, 34, 0, 41, 77, 24, 0, 23, 35, 71, 72, 24, 28, 24, 33, 35, 24, 72, 71, 77, 16, 0, 35, 77, 71, 24, 46, 0, 24, 18, 0, 35, 74, 13, 41, 3, 77, 24, 72, 75, 32, 42, 35, 36, 57, 24, 77, 71, 16, 13, 72, 13, 46, 42, 77, 46, 24, 77, 41, 31, 16, 13, 46, 54, 24, 66, 75, 46, 24, 2, 54, 72, 42, 77, 46, 0, 16, 54, 71, 72, 24, 74, 42, 74, 35, 3, 46, 24, 35, 71, 71, 74, 24, 13, 35, 41, 24, 0, 60, 24, 42, 46, 22, 15, 15, 49, 11, 11, 0, 72, 74, 42, 35, 36, 24, 24, 49, 24, 65, 0, 75, 46, 54, 24, 2, 13, 72, 0, 32, 42, 35, 13, 24, 77, 54, 71, 72, 42, 60, 60, 24, 11, 13, 35, 74, 42, 74, 13, 46, 71, 24, 42, 77, 24, 60, 71, 77, 77, 42, 35, 36, 24, 75, 16, 24, 46, 24, 30, 54, 42, 32, 71, 24, 54, 71, 24, 31, 13, 41, 24, 77, 46, 42, 32, 32, 24, 11, 0, 35, 77, 42, 74, 71, 72, 24, 54, 42, 31, 77, 71, 32, 60, 24, 13, 24, 77, 46, 75, 74, 71, 35, 46, 57, 24, 49, 72, 72]\n",
      "----------------------------------------------------\n",
      "data length :  6935\n"
     ]
    }
   ],
   "source": [
    "# one hot encode\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) # \"enumerate\" retruns index and value. Convert it to dictionary\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char_to_int)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(int_to_char)\n",
    "print(\"----------------------------------------------------\")\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[i] for i in article_text] # \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
    "print(integer_encoded)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"data length : \", len(integer_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcpMSWDHFowT"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "iteration = 1000\n",
    "sequence_length = 40\n",
    "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkqoN86qWaI4"
   },
   "source": [
    "#### Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imfg_Ew0WdDL"
   },
   "outputs": [],
   "source": [
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars, 1)) \n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)        \n",
    "\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zm6qwNiqWdMe"
   },
   "source": [
    "#### Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81qBiz_xWenI"
   },
   "outputs": [],
   "source": [
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1)\n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8sBvcdbWfhi"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "iA4RM70LWgO_",
    "outputId": "0fd64bca-f1b5-4be1-9e80-076308365598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 74.190519\n",
      "iter 100, loss: 37.900845\n",
      "iter 200, loss: 36.360056\n",
      "iter 300, loss: 27.654179\n",
      "iter 400, loss: 24.841986\n",
      "iter 500, loss: 36.668507\n",
      "iter 600, loss: 23.718021\n",
      "iter 700, loss: 18.434270\n",
      "iter 800, loss: 14.818304\n",
      "iter 900, loss: 18.020027\n",
      "CPU times: user 4h 21min 12s, sys: 10h 15min 47s, total: 14h 36min 59s\n",
      "Wall time: 2h 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = [char_to_int[ch] for ch in article_text[data_pointer:data_pointer+sequence_length]]\n",
    "        targets = [char_to_int[ch] for ch in article_text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= len(article_text) and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjh8Ip68WgYV"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HDCxDNPG68Hx"
   },
   "outputs": [],
   "source": [
    "def predict(test_char, length):\n",
    "    x = np.zeros((num_chars, 1)) \n",
    "    x[char_to_int[test_char]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((hidden_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(W_xh, x) + np.dot(W_hh, h) + b_h) \n",
    "        y = np.dot(W_hy, h) + b_y\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) \n",
    "        ix = np.random.choice(range(num_chars), p=p.ravel()) # ravel -> rank0\n",
    "        # \"ix\" is a list of indexes selected according to the soft max probability.\n",
    "        x = np.zeros((num_chars, 1)) # init\n",
    "        x[ix] = 1 \n",
    "        ixes.append(ix) # list\n",
    "    txt = test_char + ''.join(int_to_char[i] for i in ixes)\n",
    "    print ('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nGVhl-Gxh6N6",
    "outputId": "e0c8b70b-fb50-4000-f4f8-a572539513db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " The h019, f \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "predict('T', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPsz-oefL1kP"
   },
   "source": [
    "Well... that's *vaguely* language-looking. Can you do better?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
